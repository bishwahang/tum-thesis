%!TEX root = ../main.tex
\chapter{Implementation}\label{chapter:implementation}
\section{Study of current Database}
Our main resource for the research was direct access the millions of Malware samples from Anubis collected over the years. Not only was this resource our main strength but it was also a challenge to effectively and efficiently analyze those data for our research works.\\
We started with studying the Anubis system and current database. We primarily dealt with two database of Anubis backend, the \textbf{db\_report} and \textbf{web\_analysis}.
The web analysis was the first entry of any sample malware submitted for analysis. It consisted of tables such as result, file, file\_task. Each sample was given a unique file\_id along with md5 and sha hashes.
A new file\_task \emph{id} was created  and the analysis of the sample would be done for different behavioral activities related to resources such as File, Registry, Mutex activities.
These activities were saved in db\_report database table. The resource activities of the malware in \textbf{db\_report} database was associated with the \textbf{web\_analysis} database by the constraint key \textit{result\_id} of \emph{result} table in web\_analysis database.
\begin{lstlisting}[language=sql,caption={sql showing database structure to get file created activities of a malware},label={lst:resultidsql}]
SELECT result_id FROM web_analysis.task join web_analysis.file_task using (task_id) join web_analysis.file using (file_id) WHERE task_id=result_id;
SELECT name from db_report.file_created join db_report.file_name using (file_name_id) where result_id ='12345';
\end{lstlisting}
We looked upon 3 resource type for the beginning. The resources type were \textit{File, Registry, Mutex}. For each resource type we took into account their \textit{create, read, delete} activities. The total number of malware sample that we had in our test database was \textbf{22154180}.\\
Our first step was to create a reverse index from the database so that we could get the list of malware that is associated with the resource activities. We started with writing a python script to do the task and save it in the file.
However we found the first hurdle of our project. Since the number of malware were too large trying to save all their activities in data structure would cause our machine to run out of memory and hence crash the system. To solve this we took the approach of map reduce.\\
We ran our script in batch 50K malware at a time and saved the reverse index of activity to the file with numbering. Around 420 files were created for each resource type. The result files were in the format where resource name and list of result ids were separated by commas (,) as delimiter.
This was time consuming operation and it took almost 5 days for the script to finish. After the reverse index was generated in multiple numbered output, we sorted the file on resource name, alphabetically, and then joined the different parts with respect to the resource activity as key, and merged it into one single file.
Since, this was a merge sort, we considered two sorted files at a time until single file was left, the reduce part was pretty fast (Big O $O( n * \log(n))$).

\begin{lstlisting}[numbers=none]
  LANG=en_EN sort -t, -k 1,1 $file_name
  LANG=en_EN join -t , -a1 -a2 $file_name
\end{lstlisting}
The snippet of resultant output:
\begin{lstlisting}[numbers=none]
Application Data,87623151,87079727,87034095
AutoHotkey,87623151,87079727,87034095
AutoScriptWriter,87623151,87079727,87034095
B:\mbr.exe,121858971
BIN,177509111,103858187
Base Images,189524063,184501719,87504631,86763863
Buttons,111448211
C:/DOCUME~1/ADMINI~1/LOCALS~1/Temp/Adobe Reader 8,178046895,174206059,183601891,89650247
C:/DOCUME~1/ADMINI~1/LOCALS~1/Temp/_MEI1192/,161552035,116241803
\end{lstlisting}

\section{Packer \& Unpacker}
\label{sec:packerunpacker}
In order to run the pair of malware together in side the Anubis environment we made a Win32 console application, named Unpacker, as Anubis VM was Windows XP based. The notion behind this Unpacker binary is like a dropper, where a single binary will unpack itself to create two binary and run each of them.
We wrote a packer script that would add the candidate binary pair at the end of Unpacker binary and then further append the time delay and file size of each candidate binary as meta information.
The unpacker binary when executed would read itself to fetch the meta information and then the candidate binary bytes.\\
\begin{figure}[htbp]
\begin{center}
  \includegraphics[scale=0.5]{figures/unpacker.png}
\end{center}
\caption{[Packer structure] Structure of the Packer binary that would create the candidate pair and run them with delay.}
\label{fig:unpacker}
\end{figure}
We used a struct of 3 integer size to read and hold the meta information. The size of last three \textit{unsigned int} byte had the binary pair sizes information and delay, know as meta information. The offset would be deduction of $3 * \textit{size of uint}$ from the total size of itself. When the file size of packed binary was known, we used \texttt{fseek\(\)} function appropriately to start reading form the correct position until the exact size of binary were read. Then these bytes were saved to the new file pointer. Once the binary pairs were extracted by the unpacker new binary files were created in the Anubis environment and those were executed one after another with the delay of time as given in meta information. We used \texttt{windows.h} standard libraries \emph{CreateProcess} and \emph{Sleep} function for the purpose.\\

\begin{lstlisting}[language=c,caption={snippet of unpacker.c file}, label={lst:unpacker.c}]
  /* stuct for storing meta information */
  typedef struct {
    unsigned int delay;
    unsigned int fsize1;
    unsigned int fsize2;
  }meta_info;

  /* reading the meta information and first binary */
  rfp = fopen(argv[0], "rb");
  wfp1 = fopen(fileName1, "wb");

  fseek(rfp,0,SEEK_END);
  size = ftell(rfp);
  offset = size - sizeof(meta_info);
  fseek(rfp, offset, SEEK_SET);
  fread(&info, 1, sizeof(info), rfp);

  /* calculate the unpackersize from the offset and files size. */
  unpackersize = offset - (info.fsize1 + info.fsize2);

  /* rewind back and to the point of the start of file1 */
  fseek(rfp,0,SEEK_SET);
  fseek(rfp, unpackersize, SEEK_SET);

  nread_sofar = 0;
  while (nread_sofar < info.fsize1) {
      nread = fread(buf, 1, min(info.fsize1 - nread_sofar, sizeof(buf)), rfp);
      nread_sofar += nread;
      fwrite(buf, 1, nread, wfp1);
  }
  fclose(wfp1);
 \end{lstlisting}
\section{Initial Experiment}
\label{sec:Initial Experiment}
From the reverse index of the resource activities of the Malware, we created a mapping between the created activities against the deleted or read activities.
We created a list of malware based on the common resource name as the key, where a list of malware created the resource and another list of malware delete/read the same resource.
We started looking for one to one interaction of malware to a single resource. Any resource type that was created by exactly single malware and deleted by exactly another single malware.
We looked for a set $(a, b)$, where malware `a' creates the resource \emph{r}, and malware `b' deletes the same resource, \emph{r}, and no other malware has create or delete activities on that resource \emph{r}.\\
After finding such pairs, when we ran those malware pairs in the Anubis system using our unpacker, we found lots of dropper malware causing this interaction. Binary `a' was a dropper that would create many binaries including the binary `b', and binary `b' actually read itself upon execution, which was recorded as read activity by Anubis.
After running many other candidate pairs with different interaction and analyzing the results manually, not only were we able to understand the Anubis report in depth but also found an error on our approach.\\
Our notion behind checking the malware interaction was finding candidate pairs such that one malware `a' creates some resource, `r', and another malware `b' tries to access or delete the same resource,`r'.
But since Anubis ran each submitted binary in an isolated environment, there was no way that a malware `b' would find the resource that was supposed to be created by malware `a'.
We should have been looking for failed attempt activities, where a malware unsuccessfully tried to access or delete a resource created by another malware, but the current database that we had did not had record of such failed attempt.
This lead for us to look for the alternatives were we could find log of such failed attempt activities.\\
\section{Behavioral Profile}
\label{sec:behvairoalprofile}
Previously,~\citeauthor{bayer} used Anubis system for the dynamic analysis of malware to gets it execution traces~\cite[]{bayer}.
They created a behavioral profile based on the execution traces of programs irrespective of order execution. It consisted a list of different operations operated on the different OS objects during the execution of binary.
We had these behavioral profiles saved as python pickle file in our system and we used this to recreate new database according to our need.
\begin{lstlisting}[language=TeX,caption={Behvaioral Profile sample}, label={lst:bpsample}]
  op|file|'C:\\Program Files\\Common Files\\sumbh.exe'
   create:1
   open:1
   query:1
   query_file:1
   query_information:1
   write:1

  op|registry|'HKLM\\SOFTWARE\\CLASSES\\CLSID\\{00021401-0000-0000-C000-000000000046}\\INPROCSERVER32'
   open:1
   query:1
   query_value(''):1
   query_value('InprocServer32'):0
   query_value('ThreadingModel'):1

  op|section|'BaseNamedObjects\\MSCTF.MarshalInterface.FileMap.ELE.B.FLKMG'
   create:1
   map:1
   mem_read:1
   mem_write:1
\end{lstlisting}
\section{Creation of Database}
\label{sec:Creation of Database}
Recreating the database was one of the bottleneck in our project and consumed much time. The profile files were needed to be accessed via network file system, walking through list of directories and file to find the profile pickle of malware. Not all of the profile pickle were found. We found \emph{16031518 } out of \emph{22154180} malware.
We mapped the Windows Native and Windows Api call into three category modified, accessed, and delete. We took only 8 of the resource type into account while parsing the profile files; those were File, Registry, Sync, Section, Process, Mutex, Job, and Driver. We went through the most common used API calls and then mapped them into the broad three categories as described earlier.

\begin{lstlisting}[language=ruby,caption={Mapping of WinodowsNT and Windows API call},label={lbl:ntapi}]
MODIFY_LIST = {
        file:      ["create_named_pipe", "create_mailslot", "create", "rename", "lock", "set_information", "write", "unlock", "flush_buffer", "suspend", "map", "resume"],
        registry:  ["create", "restore_key", "save_key", "map", "set_value", "set_information", "compress_key", "lock", "resume", "suspend", "mem_write"],
        process:   ["create", "set_information", "suspend", "resume", "unmap", "map"],
        job:       ["assign", "set_information"],
        driver:    ["unload"],
        section:   ["create", "map", "unmap", "mem_write", "extend", "suspend", "resume", "set_information", "release"],
        sync:      ["create", "release", "map", "set_information", "mem_write"],
        service:   ["create", "start", "control"]
        }

ACCESS_LIST = {
        file:      ["query_file", "query", "open", "query_directory", "query_information", "read", "monitor_dir", "control", "device_control", "fs_control", "query_value"],
        registry:  ["enumerate", "enumerate_value", "flush", "monitor_key", "open", "query", "query_value", "mem_read"],
        process:   ["open", "query"],
        job:       ["open", "query"],
        driver:    ["load"],
        section:   ["open", "query", "mem_read", "read", "query_file", "query_system"],
        sync:      ["open", "query"],
        service:   ["open"]
        }

DELETE_LIST = {
        file:      ["delete", "open_truncate"],
        registry:  ["delete", "delete_value"],
        process:   ["delete"],
        job:       ["delete"],
        driver:    ["delete"],
        section:   ["delete"],
        sync:      ["delete"],
        service:   ["delete"]
        }

\end{lstlisting}
We used MySql for the database and followed the previous table structure so that we could reuse our previous program on the newly created database.
The final database size was 1.2 Terrabytes. The basic CRUD operations were expensive and we hit the integer overflow for registry access activities table. We had to spent much time then expected on database tuning to make the program run faster to complete in reasonable time.
Most part of the execution time was taken to read the profile gzip pickle and then normal sql queries. This was for 17 million malware pickle profiles and their resource activities as defined above.
%TODO: put profile.png
After the creation of new database, we made a reverse index from the new refined data for the resource to malware ids.
We took into account those activity that had successful creation and failed access or delete action. We made a mapping like before and started to run the candidate pairs to test the behavioral interaction between the pairs. But, since the number of pairs were too many we hit the \emph{n-comination} problem.
A single resource \texttt{`r'} would have been created by more than thousands of malware and also same resource \texttt{`r'} would have been tried to access or delete with a failed attempt by another thousands of malware. If we made a pair out of every malware as possible candidate it would be too many candidates for us to run and analyze result.
We looked into clustering of the malware behavior to address this problem so that we could cluster malware on different topic and only choose one sample from the list of topic.
\section{Clustering}
\label{sec:Clustering}
We researched on many types of clustering algorithm that we could use to cluster our malware. Our idea of clustering was based on malware resource activities.
We had all the resource activities such as modification, access and deletion related to different resource types as discussed above.
We represented each resource type and activities as numeric value. And each resource name woul be represented by it's unique resource name id on our database system.
\begin{lstlisting}[language=python,caption={Numeric codes given to resource and operation},label={lbl:numericode}]
  RESOURCE_CODE    = {"file" : "1", "registry" : "2", "section" : "3", "service" : "4", "driver" : "5", "sync" : "6", "process" : "7", "job" : "8"}
  OPERATION_CODE   = {"access" : "1", "delete" : "2", "modify" : "3"}
\end{lstlisting}
So a file delete activity of filename, \textit{`c:\textbackslash\textbackslash{}gbot.exe'}, with file\_name\_id, ``4986'' in our database, by some malware ``A'', would be represented as single word ``1\_3\_4986''. Each such resource activities related to one single malware would be a bag of words representing  a single document representing that particular malware.\\
After days of running the script we could gather all the resource activity of \gettotalmalware2 malware. This was about 81 Gigabytes of data.
We used python module Gensim~\cite[Gensim]{gensim}  for the topic clustering and used the LDA model as described in Methodology section. We filtered our corpora by disacarding any resource activity that occurred in less than 1000 or more than 1 million for clustering.
This was based on heuristic after studying the cdf graph of activities to number of malware.
%TODO: add cdf of reverse index

The number of topic was chosen to be 1000, however our malware samples were distributed among only 752 unique topic.
We calculated the distance.
\begin{lstlisting}[language=python,caption={Script to run Gensim LDA},label={lbl:lda.py}]
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from gensim import corpora, models, similarities
import sys
document_name = sys.argv[1]
# read through the text to create dictionary
dictionary = corpora.Dictionary(line.split() for line in open(document_name))
# filtered dictionary
dictionary.filter_extremes(1000,0.6)
# removed gaps in id sequence after filtering
dictionary.compactify()
# read through the text and create corpus from dicionary
# use iterator beause of large data 80 gigs
class MyCorpus(object):
    def __iter__(self):
        for line in open(document_name):
            yield dictionary.doc2bow(line.split())

corpus = MyCorpus()
corpora.MmCorpus.serialize('1K_06_corpus.mm', corpus)
\end{lstlisting}






\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.23]{figures/maxflow.png}
  \caption[Big Picture]{Max flow implementation to find out candidates with optimal number of resource}\label{fig:maxflow}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.45]{figures/dhkherusitcs.png}
  \caption[Cluster, Malware, Resource relation chart]{Chart showing the interconnection between Cluster topics, Malware, and Resources.}\label{fig:dhkheuristics}
\end{figure}
